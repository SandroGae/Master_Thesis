{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Imports =======\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"float32\") # float 32 to test if this was the issue\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from unet_3d_data_JENS import prepare_in_memory_5to5\n",
    "from jens_stuff import SumScaleNormalizer, reset_random_seeds\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import regularizers, constraints\n",
    "\n",
    "import sys, inspect\n",
    "import json, socket, getpass, platform, subprocess, time, uuid # for naming files and callbacks\n",
    "\n",
    "seed = 0\n",
    "reset_random_seeds(seed)\n",
    "for g in tf.config.list_physical_devices('GPU'):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except: pass\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80679b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Loading Data in RAM =====\n",
    "print(\">>> Phase 1: Starting data prep on CPU...\")\n",
    "results = prepare_in_memory_5to5(\n",
    "    data_dir=Path.home() / \"data\" / \"original_data\",\n",
    "    size=5, group_len=41, dtype=np.float32,\n",
    ")\n",
    "print(\">>> Data preperation finished, all data in RAM\")\n",
    "X_train, Y_train = results[\"train\"]\n",
    "X_val,   Y_val   = results[\"val\"]\n",
    "X_test,  Y_test  = results[\"test\"]\n",
    "\n",
    "INPUT_SHAPE = X_train.shape[1:]\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb04812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Preprocessing (nach dem Laden definieren) =====\n",
    "preproc_train = SumScaleNormalizer(\n",
    "    scale_range=[5000, 15001], pre_offset=0.0,\n",
    "    normalize_label=True, axis=None, batch_mode=False,\n",
    "    clip_before=[0., np.inf], clip_after=[0., 1.]\n",
    ")\n",
    "preproc_valid = SumScaleNormalizer(\n",
    "    scale_range=[5000, 5001], pre_offset=0.0,\n",
    "    normalize_label=True, axis=None, batch_mode=False,\n",
    "    clip_before=[0., np.inf], clip_after=[0., 1.]\n",
    ")\n",
    "\n",
    "def make_ds(X, Y, shuffle=True, preproc=None):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    if preproc is not None:\n",
    "        ds = ds.map(lambda x, y: tuple(preproc.map(x, y)),\n",
    "                    num_parallel_calls=AUTO).cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=X.shape[0])\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "print(\">>> Phase 2: Create Tensorflow Datasets...\")\n",
    "train_ds = make_ds(X_train, Y_train, True,  preproc=preproc_train)\n",
    "val_ds   = make_ds(X_val,   Y_val,   False, preproc=preproc_valid)\n",
    "test_ds  = make_ds(X_test,  Y_test,  False, preproc=preproc_valid)\n",
    "print(\">>> Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Defining 3D-U-Net Architecture ========\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3,3,3), padding=\"same\", activation=None):\n",
    "    ki  = \"glorot_uniform\"\n",
    "    kr  = regularizers.l2(1e-5)                  # milde L2\n",
    "    kc  = constraints.MaxNorm(3.0)               # Max-Norm gegen Explodieren\n",
    "\n",
    "    x = layers.Conv3D(filters, kernel_size, padding=padding,\n",
    "                      kernel_initializer=ki, use_bias=True,\n",
    "                      kernel_regularizer=kr, kernel_constraint=kc)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)           # sanfter als ReLU\n",
    "\n",
    "    x = layers.Conv3D(filters, kernel_size, padding=padding,\n",
    "                      kernel_initializer=ki, use_bias=True,\n",
    "                      kernel_regularizer=kr, kernel_constraint=kc)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def unet3d(input_shape=(5, 192, 240, 1), base_filters=32):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder (pool only over H,W)\n",
    "    c1 = conv_block(inputs, base_filters)\n",
    "    p1 = layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2))(c1)\n",
    "\n",
    "    c2 = conv_block(p1, base_filters*2)\n",
    "    p2 = layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2))(c2)\n",
    "\n",
    "    c3 = conv_block(p2, base_filters*4)\n",
    "    p3 = layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = conv_block(p3, base_filters*8)\n",
    "\n",
    "    # Decoder (upsample only over H,W)\n",
    "    u3 = layers.Conv3DTranspose(base_filters*4, kernel_size=(1,2,2), strides=(1,2,2), padding=\"same\")(bn) # bottleneck\n",
    "    u3 = layers.concatenate([u3, c3])\n",
    "    c4 = conv_block(u3, base_filters*4)\n",
    "\n",
    "    u2 = layers.Conv3DTranspose(base_filters*2, kernel_size=(1,2,2), strides=(1,2,2), padding=\"same\")(c4)\n",
    "    u2 = layers.concatenate([u2, c2])\n",
    "    c5 = conv_block(u2, base_filters*2)\n",
    "\n",
    "    u1 = layers.Conv3DTranspose(base_filters, kernel_size=(1,2,2), strides=(1,2,2), padding=\"same\")(c5)\n",
    "    u1 = layers.concatenate([u1, c1])\n",
    "    c6 = conv_block(u1, base_filters)\n",
    "\n",
    "    outputs = layers.Conv3D(1, (1,1,1), dtype=\"float32\", activation=\"sigmoid\")(c6)\n",
    "    return models.Model(inputs, outputs, name=\"3D_U-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Defining Loss function MAE + MS-SSIM (slice-wise) ========\n",
    "\n",
    "ALPHA_TARGET = 0.7    # <- bis hier hochfahren\n",
    "ALPHA = 0.0\n",
    "K_SLICES     = 5\n",
    "MS_EPS       = 1e-5\n",
    "\n",
    "# ALPHA als TF-Variable (damit der Graph nicht einfriert)\n",
    "ALPHA_TF = tf.Variable(0.0, trainable=False, dtype=tf.float32, name=\"alpha_ms_ssim\")\n",
    "\n",
    "\n",
    "def _clip01(x):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    return tf.clip_by_value(x, 0.0, 1.0)\n",
    "\n",
    "def _safe_imgs_for_ms_ssim(y_true, y_pred):\n",
    "    yt = _clip01(y_true); yp = _clip01(y_pred)\n",
    "    yt = tf.clip_by_value(yt, MS_EPS, 1.0 - MS_EPS)\n",
    "    yp = tf.clip_by_value(yp, MS_EPS, 1.0 - MS_EPS)\n",
    "    tf.debugging.assert_all_finite(yt, \"y_true contains NaN/Inf\")\n",
    "    tf.debugging.assert_all_finite(yp, \"y_pred contains NaN/Inf\")\n",
    "    return yt, yp\n",
    "\n",
    "def ms_ssim_loss_sampled(y_true, y_pred, k=K_SLICES):\n",
    "    yt, yp = _safe_imgs_for_ms_ssim(y_true, y_pred)\n",
    "    B = tf.shape(yt)[0]; D = tf.shape(yt)[1]\n",
    "    idx = _sample_depth_indices(B, D, k=k)\n",
    "    ygt = tf.gather(yt, idx, batch_dims=1)\n",
    "    ypd = tf.gather(yp, idx, batch_dims=1)\n",
    "    yt2 = tf.reshape(ygt, (-1, tf.shape(yt)[2], tf.shape(yt)[3], tf.shape(yt)[4]))\n",
    "    yp2 = tf.reshape(ypd, (-1, tf.shape(yp)[2], tf.shape(yp)[3], tf.shape(yp)[4]))\n",
    "    ms = tf.image.ssim_multiscale(yt2, yp2, max_val=1.0)\n",
    "    tf.debugging.assert_all_finite(ms, \"MS-SSIM produced NaN/Inf\")\n",
    "    return 1.0 - tf.reduce_mean(ms)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    yt = _clip01(y_true); yp = _clip01(y_pred)\n",
    "    l_mae = tf.reduce_mean(tf.abs(yt - yp))\n",
    "    def ms_branch():\n",
    "        l_ms = ms_ssim_loss_sampled(yt, yp, k=K_SLICES)\n",
    "        return (1.0 - ALPHA_TF) * l_mae + ALPHA_TF * l_ms\n",
    "    def mae_branch():\n",
    "        return l_mae\n",
    "    return tf.cond(ALPHA_TF > 0.0, ms_branch, mae_branch)\n",
    "\n",
    "def mae_metric(y_true, y_pred):\n",
    "    yt = _clip01(y_true); yp = _clip01(y_pred)\n",
    "    return tf.reduce_mean(tf.abs(yt - yp))\n",
    "\n",
    "def ms_ssim_metric(y_true, y_pred):\n",
    "    yt, yp = _safe_imgs_for_ms_ssim(y_true, y_pred)\n",
    "    yt2 = tf.reshape(yt, (-1, tf.shape(yt)[2], tf.shape(yt)[3], tf.shape(yt)[4]))\n",
    "    yp2 = tf.reshape(yp, (-1, tf.shape(yp)[2], tf.shape(yp)[3], tf.shape(yp)[4]))\n",
    "    return tf.reduce_mean(tf.image.ssim_multiscale(yt2, yp2, max_val=1.0))\n",
    "def psnr_metric(y_true, y_pred):\n",
    "    yt = _clip01(y_true); yp = _clip01(y_pred)\n",
    "    return tf.image.psnr(yt, yp, max_val=1.0)\n",
    "\n",
    "def _sample_depth_indices(batch_size, depth, k=1, seed=42):\n",
    "    \"\"\"\n",
    "    Generates deterministic matrix and samples indices using highest values per row\n",
    "    \"\"\"\n",
    "    rnd = tf.random.stateless_uniform([batch_size, depth], seed=[seed, 0]) # (B,D) matrix with random values\n",
    "    topk = tf.math.top_k(rnd, k=k).indices                                 # Search for 2 highest values per row\n",
    "    return topk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Automaic Naming Pipeline ============\n",
    "\n",
    "def _safe_git_commit():\n",
    "    try:\n",
    "        return subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _serialize_optimizer(opt):\n",
    "    try:\n",
    "        return tf.keras.optimizers.serialize(opt)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _timestamp():\n",
    "    # Dateiname-sicher (kein \":\" unter Windows)\n",
    "    return time.strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "class BestFinalizeCallback(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Am Ende:\n",
    "      1) nimmt die von ModelCheckpoint geschriebene TEMP-Datei und benennt sie um zu <code>_NEW_valloss_...>.keras\n",
    "      2) schreibt JSON\n",
    "      3) rankt *alle* <code>_*.keras strikt nach val_loss -> <code>_V1_..., <code>_V2_..., ... (Luecken werden beseitigt)\n",
    "    \"\"\"\n",
    "    def __init__(self, root: Path, run_meta: dict = None, tmp_name: str = None, code_name: str = None):\n",
    "        super().__init__()\n",
    "        self.root = Path(root); self.root.mkdir(parents=True, exist_ok=True)\n",
    "        auto = self._auto_code_name() if (code_name is None or str(code_name).upper() == \"AUTO\") else code_name\n",
    "        self.code = self._sanitize_code(auto)\n",
    "        self.tmp_path = self.root / (tmp_name or f\"{self.code}_TEMP_{uuid.uuid4().hex}.keras\")\n",
    "        self.best_val_loss = np.inf\n",
    "        self.best_psnr = None\n",
    "        self.run_meta = run_meta or {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _sanitize_code(code: str) -> str:\n",
    "        safe = \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in (code or \"\").strip())\n",
    "        return safe or \"MODEL\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _auto_code_name():\n",
    "        # 1) __main__.__file__\n",
    "        try:\n",
    "            main_mod = sys.modules.get(\"__main__\")\n",
    "            if main_mod and getattr(main_mod, \"__file__\", None):\n",
    "                return os.path.splitext(os.path.basename(main_mod.__file__))[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # 2) sys.argv[0]\n",
    "        try:\n",
    "            if sys.argv and sys.argv[0]:\n",
    "                return os.path.splitext(os.path.basename(sys.argv[0]))[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # 3) erster echter Stack-Frame\n",
    "        try:\n",
    "            for fr in inspect.stack():\n",
    "                fn = fr.filename\n",
    "                if fn and fn not in (\"<stdin>\", \"<string>\"):\n",
    "                    return os.path.splitext(os.path.basename(fn))[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # 4) SLURM/PBS\n",
    "        for k in (\"SLURM_JOB_NAME\", \"PBS_JOBNAME\", \"JOB_NAME\"):\n",
    "            v = os.environ.get(k)\n",
    "            if v:\n",
    "                return v\n",
    "        return \"MODEL\"\n",
    "\n",
    "    # ---------- Trainingslogik: nur Metriken merken, KEIN Speichern hier! ----------\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not logs or \"val_loss\" not in logs:\n",
    "            return\n",
    "        vloss = float(logs[\"val_loss\"])\n",
    "        if vloss < self.best_val_loss:\n",
    "            self.best_val_loss = vloss\n",
    "            psnr = logs.get(\"psnr_metric\")\n",
    "            self.best_psnr = float(psnr) if psnr is not None else None\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # TEMP -> NEW_* (nur wenn TEMP existiert – ModelCheckpoint muss sie geschrieben haben)\n",
    "        vloss_str = f\"{self.best_val_loss:.3e}\" if np.isfinite(self.best_val_loss) else \"nan\"\n",
    "        psnr_part = f\"_PSNR_{self.best_psnr:.3g}\" if (self.best_psnr is not None and np.isfinite(self.best_psnr)) else \"\"\n",
    "        new_model = self.root / f\"{self.code}_NEW_valloss_{vloss_str}{psnr_part}.keras\"\n",
    "\n",
    "        if self.tmp_path.exists() and self.tmp_path.stat().st_size > 0:\n",
    "            try:\n",
    "                os.replace(self.tmp_path, new_model)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Konnte TEMP nicht nach NEW umbenennen: {e}\")\n",
    "                return\n",
    "            # JSON schreiben\n",
    "            self._write_json_for_model(new_model)\n",
    "\n",
    "        # Re-Ranking fuer alle mit diesem Prefix\n",
    "        self._rank_all_models()\n",
    "\n",
    "        # Aufraeumen\n",
    "        try:\n",
    "            if self.tmp_path.exists():\n",
    "                os.remove(self.tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---------- JSON ----------\n",
    "    def _write_json_for_model(self, model_path: Path):\n",
    "        # kein Timestamp mehr im Dateinamen\n",
    "        json_path = model_path.with_suffix(\".json\")\n",
    "\n",
    "        try:\n",
    "            inp_shape = tuple(int(x) for x in (self.model.input_shape or []) if isinstance(x, (int,np.integer)))\n",
    "        except Exception:\n",
    "            inp_shape = None\n",
    "        try:\n",
    "            loss_name = getattr(self.model.loss, \"__name__\", str(self.model.loss))\n",
    "        except Exception:\n",
    "            loss_name = None\n",
    "        try:\n",
    "            metrics_list = [getattr(m, \"__name__\", str(m)) for m in (self.model.metrics or [])]\n",
    "        except Exception:\n",
    "            metrics_list = None\n",
    "\n",
    "        meta = {\n",
    "            \"timestamp\": _timestamp(),   # bleibt im JSON-Inhalt erhalten!\n",
    "            \"user\": getpass.getuser(),\n",
    "            \"host\": socket.gethostname(),\n",
    "            \"platform\": platform.platform(),\n",
    "            \"git_commit\": _safe_git_commit(),\n",
    "            \"code_name\": self.code,\n",
    "            \"batch_size\": self.run_meta.get(\"batch_size\"),\n",
    "            \"epochs_planned\": self.run_meta.get(\"epochs\"),\n",
    "            \"early_stopping\": self.run_meta.get(\"early_stopping\"),\n",
    "            \"data_prep\": self.run_meta.get(\"data_prep\"),\n",
    "            \"alpha_ms_ssim\": self.run_meta.get(\"ALPHA\"),\n",
    "            \"best_val_loss\": float(self.best_val_loss) if np.isfinite(self.best_val_loss) else None,\n",
    "            \"best_psnr_metric\": self.best_psnr,\n",
    "            \"input_shape\": inp_shape,\n",
    "            \"loss\": loss_name,\n",
    "            \"metrics\": metrics_list,\n",
    "            \"optimizer\": _serialize_optimizer(getattr(self.model, \"optimizer\", None)),\n",
    "            \"mixed_precision_policy\": mixed_precision.global_policy().name if mixed_precision.global_policy() else None,\n",
    "        }\n",
    "        try:\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(meta, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Konnte JSON nicht schreiben: {e}\")\n",
    "\n",
    "    # ---------- Parsing ----------\n",
    "    @staticmethod\n",
    "    def _parse_filename_simple(name: str):\n",
    "        if not name.endswith(\".keras\"):\n",
    "            return None\n",
    "        base = name[:-6]\n",
    "        parts = base.split(\"_\")\n",
    "        try:\n",
    "            i_vl = parts.index(\"valloss\")\n",
    "        except ValueError:\n",
    "            return None\n",
    "        if i_vl + 1 >= len(parts):\n",
    "            return None\n",
    "        try:\n",
    "            val_loss = float(parts[i_vl + 1])\n",
    "        except Exception:\n",
    "            return None\n",
    "        psnr = None\n",
    "        try:\n",
    "            i_ps = parts.index(\"PSNR\")\n",
    "            if i_ps + 1 < len(parts):\n",
    "                psnr = float(parts[i_ps + 1])\n",
    "        except ValueError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            psnr = None\n",
    "        return {\"val_loss\": val_loss, \"psnr\": psnr}\n",
    "\n",
    "    # ---------- Ranking & Umbenennen ----------\n",
    "    def _rank_all_models(self):\n",
    "        items = []\n",
    "        for p in self.root.glob(f\"{self.code}_*.keras\"):\n",
    "            if not p.is_file():\n",
    "                continue\n",
    "            meta = self._parse_filename_simple(p.name)\n",
    "            if meta:\n",
    "                items.append((p, meta[\"val_loss\"], meta[\"psnr\"]))\n",
    "        if not items:\n",
    "            return\n",
    "        items.sort(key=lambda x: (x[1], x[0].stat().st_mtime))\n",
    "\n",
    "        temps = []\n",
    "        for path, vloss, psnr in items:\n",
    "            base_stem = path.with_suffix(\"\").name\n",
    "            jsons = []\n",
    "            p0 = self.root / (base_stem + \".json\")\n",
    "            if p0.exists():\n",
    "                jsons.append(p0)\n",
    "            t_model = self.root / f\".tmp_{uuid.uuid4().hex}.keras\"\n",
    "            os.replace(path, t_model)\n",
    "            tmp_jsons = []\n",
    "            for j in jsons:\n",
    "                ts_suffix = j.name[len(base_stem):]\n",
    "                t_json = t_model.with_suffix(\"\")\n",
    "                t_json = t_json.parent / (t_json.name + ts_suffix)\n",
    "                os.replace(j, t_json)\n",
    "                tmp_jsons.append((t_json, ts_suffix))\n",
    "            temps.append((t_model, tmp_jsons, vloss, psnr))\n",
    "\n",
    "        for rank, (t_model, tmp_jsons, vloss, psnr) in enumerate(temps, start=1):\n",
    "            v = f\"{vloss:.3e}\"\n",
    "            ps = f\"_PSNR_{psnr:.3g}\" if psnr is not None else \"\"\n",
    "            final_model = self.root / f\"{self.code}_V{rank}_valloss_{v}{ps}.keras\"\n",
    "            os.replace(t_model, final_model)\n",
    "            final_stem = final_model.with_suffix(\"\").name\n",
    "            for t_json, ts_suffix in tmp_jsons:\n",
    "                if t_json.exists():\n",
    "                    final_json = final_model.with_suffix(\"\")\n",
    "                    final_json = final_json.parent / (final_stem + ts_suffix)\n",
    "                    os.replace(t_json, final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Train (STEP 1: MAE-only sanity) ========\n",
    "print(\">>> Phase 3: GPU training starts now!\")\n",
    "\n",
    "# ======== Alpha linear hochfahren ========\n",
    "class AlphaScheduler(callbacks.Callback):\n",
    "    def __init__(self, warmup=1, step=0.1, target=ALPHA_TARGET, min_alpha=0.0):\n",
    "        super().__init__()\n",
    "        self.warmup = int(warmup)\n",
    "        self.step   = float(step)\n",
    "        self.target = float(target)\n",
    "        self.min_a  = float(min_alpha)\n",
    "        self.best_val = np.inf\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # während Warmup bei 0.0 bleiben\n",
    "        if epoch < self.warmup:\n",
    "            ALPHA_TF.assign(self.min_a)\n",
    "        print(f\"[AlphaScheduler] begin epoch {epoch}  ALPHA={float(ALPHA_TF.numpy()):.3f}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        vl = float(logs.get(\"val_loss\", np.inf))\n",
    "        if not np.isfinite(vl):\n",
    "            # instabil -> alpha leicht senken\n",
    "            new_a = max(self.min_a, float(ALPHA_TF.numpy()) - self.step/2.0)\n",
    "            ALPHA_TF.assign(new_a)\n",
    "            print(f\"[AlphaScheduler] val_loss non-finite -> ALPHA={new_a:.3f}\")\n",
    "            return\n",
    "\n",
    "        improved = vl < self.best_val - 1e-6\n",
    "        if improved:\n",
    "            self.best_val = vl\n",
    "\n",
    "        # Wenn stabil (endlich) und nicht deutlich schlechter, erhöhe ALPHA\n",
    "        if vl <= self.best_val * 1.02:  # <= +2% Verschlechterung tolerieren\n",
    "            new_a = min(self.target, float(ALPHA_TF.numpy()) + self.step)\n",
    "            ALPHA_TF.assign(new_a)\n",
    "            print(f\"[AlphaScheduler] val ok -> ALPHA={new_a:.3f}\")\n",
    "        else:\n",
    "            # leichte Verschlechterung -> halte oder kleine Reduktion\n",
    "            new_a = max(self.min_a, float(ALPHA_TF.numpy()) - self.step/2.0)\n",
    "            ALPHA_TF.assign(new_a)\n",
    "            print(f\"[AlphaScheduler] val worsened -> ALPHA={new_a:.3f}\")\n",
    "\n",
    "# ======== Checkpoints inkl. BestFinalize ========\n",
    "ckpt_root = Path.home() / \"data\" / \"checkpoints_3d_unet\"\n",
    "run_meta = {\n",
    "    \"batch_size\": BATCH_SIZE, \"epochs\": EPOCHS,\n",
    "    \"early_stopping\": {\"monitor\":\"val_loss\",\"patience\":10},\n",
    "    \"data_prep\": {\"size\": 5, \"group_len\": 41, \"dtype\": \"float32\"},\n",
    "    \"ALPHA\": ALPHA\n",
    "}\n",
    "\n",
    "bf = BestFinalizeCallback(ckpt_root, run_meta=run_meta, code_name=\"AUTO\")\n",
    "ckpt_best = callbacks.ModelCheckpoint(\n",
    "    filepath=str(bf.tmp_path), monitor=\"val_loss\",\n",
    "    mode=\"min\", save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "# ======== Callbacks-Liste ========\n",
    "\n",
    "model = unet3d(input_shape=INPUT_SHAPE, base_filters=16)\n",
    "\n",
    "# ======== Optimizer (etwas konservativer) ========\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-5,       # notfalls 5e-6\n",
    "    epsilon=1e-6,\n",
    "    global_clipnorm=1.0,      # GANZ WICHTIG: global, nicht nur per-layer\n",
    "    clipvalue=0.5             # optional: zusätzlich kappen\n",
    ")\n",
    "model.compile(optimizer=opt, loss=combined_loss,\n",
    "              metrics=[\"mae\", psnr_metric, ms_ssim_metric], jit_compile=False)\n",
    "\n",
    "model.run_eagerly = True  # nur zum Debuggen, danach wieder False\n",
    "\n",
    "class LogAlphaAtEnd(callbacks.Callback):\n",
    "    def on_train_end(self, logs=None):\n",
    "        try:\n",
    "            bf.run_meta[\"ALPHA_final\"] = float(ALPHA_TF.numpy())\n",
    "            bf.run_meta[\"ALPHA_target\"] = float(ALPHA_TARGET)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "class WeightNaNGuard(callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        for i, w in enumerate(self.model.weights):\n",
    "            if not tf.reduce_all(tf.math.is_finite(w)):\n",
    "                print(f\"[NaNWeight] layer={w.name} batch={batch}\")\n",
    "                self.model.stop_training = True\n",
    "                break\n",
    "\n",
    "cbs = [\n",
    "    AlphaScheduler(warmup=2, step=0.05, target=ALPHA_TARGET),\n",
    "    WeightNaNGuard(),\n",
    "    LogAlphaAtEnd(),                 # <- HINZU\n",
    "    callbacks.TerminateOnNaN(),\n",
    "    ckpt_best, bf,\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "]\n",
    "\n",
    "# ======== Train ========\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,   # z.B. 6-10 empfehlenswert fuer den Ramp\n",
    "    callbacks=cbs,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "print(\">>> Phase 3: Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
