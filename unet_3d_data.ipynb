{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed022219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Imports ===\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedda05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Functions for Normalizing Data =================\n",
    "def anscombe_vst(x):\n",
    "    \"\"\"\n",
    "    Negative values get yeeted to zero (counts should not be negative)\n",
    "    \"\"\"\n",
    "    x = np.maximum(x, 0)\n",
    "    return 2.0 * np.sqrt(x + 3.0/8.0)\n",
    "\n",
    "def compute_clip_from_high(high_data, percentile=99.9, use_vst=True, max_samples=5_000_000, rng=None):\n",
    "    \"\"\"\n",
    "    From High count data, determine global Clip_values\n",
    "    percentile: e.g. 99.9\n",
    "    use_vst: If True -> Calculate Clip on VSC domain (usually better)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    arr = high_data.ravel()\n",
    "    sample = arr if arr.size <= max_samples else arr[rng.choice(arr.size, size=max_samples, replace=False)]\n",
    "    if use_vst:\n",
    "        sample = anscombe_vst(sample)\n",
    "    clip_val = np.percentile(sample, percentile)\n",
    "    if not np.isfinite(clip_val) or clip_val <= 0:\n",
    "        clip_val = float(np.max(sample))\n",
    "    return float(clip_val)\n",
    "\n",
    "def preprocess_counts(x, clip_val, use_vst=True, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Normalization: optional VST -> clip -> /clip -> [0,1]\n",
    "    \"\"\"\n",
    "    if use_vst:\n",
    "        x = anscombe_vst(x)\n",
    "    x = np.clip(x, 0, clip_val) / clip_val\n",
    "    return x.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53870a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= Function for building 3D Datasets =================\n",
    "\n",
    "def build_sequential_dataset(low_data, high_data, size, group_len, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Generates training data:\n",
    "      X: (B, size, H, W) = window of `size` Low-Count images\n",
    "      Y: (B, size, H, W) = Ground truth = window of `size` High-Count images (3D output)\n",
    "      N: Number of Pictrues in total\n",
    "      H: Height of each image\n",
    "      W: Width of each image\n",
    "      size: Size of sliding window (must be odd)\n",
    "      group_len: Length of each block (41 for training/test/val)\n",
    "    \"\"\"\n",
    "    assert low_data.shape == high_data.shape, \"low/high must have identical shapes\"\n",
    "    N, H, W = low_data.shape\n",
    "    if size % 2 == 0 or size < 1:\n",
    "        raise ValueError(\"`size` must be odd and >= 1 (e.g., 3, 5, 7)\")\n",
    "    if N % group_len != 0:\n",
    "        raise ValueError(f\"N={N} is not a multiple of group_len={group_len}.\")\n",
    "\n",
    "    num_groups = N // group_len\n",
    "    X_list, Y_list = [], []\n",
    "\n",
    "    for group_index in range(num_groups):\n",
    "        start = group_index * group_len\n",
    "        end   = start + group_len\n",
    "        # slide window inside this block only\n",
    "        for n in range(start, end - size + 1):         # stride = 1\n",
    "            X_list.append(low_data[n: n + size])       # (size,H,W)\n",
    "            Y_list.append(high_data[n: n + size])      # (size,H,W)\n",
    "\n",
    "    X = np.stack(X_list, axis=0).astype(dtype)   # (B, size, H, W)\n",
    "    Y = np.stack(Y_list, axis=0).astype(dtype)   # (B, size, H, W)\n",
    "    # Adding Channel-Dimension since PyTorch expects (B,C,D,H,W) with C=1\n",
    "    X = X[..., None]  # (B, size, H, W, 1)\n",
    "    Y = Y[..., None]  # (B, size, H, W, 1)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcda27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_in_memory_5to5\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39m\u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      4\u001b[0m     group_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m41\u001b[39m,\n\u001b[0;32m      5\u001b[0m     use_vst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# F\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     percentile\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99.9\u001b[39m,\n\u001b[0;32m      7\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m      8\u001b[0m ):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load\u001b[39m(fp):\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_in_memory_5to5(\n",
    "    data_dir=Path(\"data\") / \"original_data\",\n",
    "    size=5,\n",
    "    group_len=41,\n",
    "    use_vst=False, # Activates / Deactivates Anscombe transform\n",
    "    percentile=99.9,\n",
    "    dtype=np.float32,\n",
    "):\n",
    "    def _load(fp):\n",
    "        with h5py.File(fp, \"r\") as f:\n",
    "            high = f[\"/high_count/data\"][:].transpose(2,0,1)\n",
    "            low  = f[\"/low_count/data\"][:].transpose(2,0,1)\n",
    "        return high, low\n",
    "\n",
    "    data = {\n",
    "        \"train\": _load(data_dir / \"training_data.hdf5\"),\n",
    "        \"test\":  _load(data_dir / \"test_data.hdf5\"),\n",
    "        \"val\":   _load(data_dir / \"validation_data.hdf5\"),\n",
    "    }\n",
    "\n",
    "    high_train, _ = data[\"train\"]\n",
    "    clip_val_train = compute_clip_from_high(high_train, percentile=percentile, use_vst=use_vst)\n",
    "\n",
    "    results = {}\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        high_split, low_split = data[split]\n",
    "        low_n  = preprocess_counts(low_split,  clip_val_train, use_vst=use_vst, dtype=dtype)\n",
    "        high_n = preprocess_counts(high_split, clip_val_train, use_vst=use_vst, dtype=dtype)\n",
    "        X, Y = build_sequential_dataset(low_n, high_n, size=size, group_len=group_len, dtype=dtype)\n",
    "        results[split] = (X, Y)\n",
    "        del low_n, high_n\n",
    "        gc.collect()\n",
    "\n",
    "    return results, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ===== Visualization of some samples =====\n",
    "\n",
    "def show_window_pair_3d(X, Y, sample_idx, size=5, group_len=41):\n",
    "\n",
    "    seq_low  = X[sample_idx, ..., 0]   # (size,H,W)\n",
    "    seq_high = Y[sample_idx, ..., 0]   # (size,H,W)\n",
    "\n",
    "    k = size // 2\n",
    "    group_idx = sample_idx // (group_len - size + 1)\n",
    "    offset_in_group = sample_idx % (group_len - size + 1)\n",
    "    global_start = group_idx * group_len + offset_in_group\n",
    "    frame_indices = list(range(global_start, global_start + size))\n",
    "\n",
    "    fig, axes = plt.subplots(2, size, figsize=(3 * size, 6))\n",
    "    for j in range(size):\n",
    "        v1, v2 = np.percentile(seq_low[j].ravel(), (1,99))\n",
    "        im_low = axes[0, j].imshow(seq_low[j], cmap=\"gray_r\", origin=\"lower\",\n",
    "                                   aspect=\"equal\", vmin=v1, vmax=v2)\n",
    "        axes[0, j].set_title(f\"Low idx={frame_indices[j]}\"); axes[0, j].axis(\"off\")\n",
    "        fig.colorbar(im_low, ax=axes[0, j], fraction=0.046, pad=0.04)\n",
    "\n",
    "        v1, v2 = np.percentile(seq_high[j].ravel(), (1,99))\n",
    "        im_high = axes[1, j].imshow(seq_high[j], cmap=\"gray_r\", origin=\"lower\",\n",
    "                                    aspect=\"equal\", vmin=v1, vmax=v2)\n",
    "        axes[1, j].set_title(f\"High idx={frame_indices[j]}\"); axes[1, j].axis(\"off\")\n",
    "        fig.colorbar(im_high, ax=axes[1, j], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[0, k].set_title(f\"Low idx={frame_indices[k]} (center)\")\n",
    "    axes[1, k].set_title(f\"High idx={frame_indices[k]} (center)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Visualizing examples:\n",
    "X_vis, Y_vis = results[\"train\"]\n",
    "for idx in range(3):\n",
    "    show_window_pair_3d(X_vis, Y_vis, sample_idx=idx, size=size, group_len=group_len)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
