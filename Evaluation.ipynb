{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc70fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# === Import deiner Daten-Pipeline ===\n",
    "from unet_3d_data import prepare_in_memory_5to5\n",
    "\n",
    "# ====== Utils: Anscombe (nur fuer Metriken im Originalraum) ======\n",
    "def inv_anscombe_tf(z):\n",
    "    z = tf.maximum(z, 1e-3)\n",
    "    z2 = z / 2.0\n",
    "    y = z2**2 - 1.0/8.0 + 1.0/(4.0*z**2) - 11.0/(8.0*z**4)\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "# ====== Auswahl: Checkpoint-Ordner + Modellversion ======\n",
    "DATA_ROOT = Path.home() / \"data\"\n",
    "\n",
    "def pick_checkpoint_dir():\n",
    "    cand = sorted([p for p in DATA_ROOT.iterdir()\n",
    "                   if p.is_dir() and p.name.startswith(\"checkpoints_\")])\n",
    "    if not cand:\n",
    "        print(\"Keine Checkpoint-Ordner gefunden unter ~/data (checkpoints_*)\")\n",
    "        sys.exit(1)\n",
    "    print(\"Waehle Checkpoint-Ordner:\")\n",
    "    for i, p in enumerate(cand, 1):\n",
    "        print(f\"  [{i}] {p.name}\")\n",
    "    while True:\n",
    "        s = input(\"Nummer: \").strip()\n",
    "        if s.isdigit() and 1 <= int(s) <= len(cand):\n",
    "            return cand[int(s)-1]\n",
    "\n",
    "def pick_version(ckpt_dir: Path):\n",
    "    models = []\n",
    "    pat = re.compile(r\"^V(\\d+)_.*\\.keras$\")\n",
    "    for p in ckpt_dir.iterdir():\n",
    "        if p.is_file() and p.suffix == \".keras\" and pat.match(p.name):\n",
    "            models.append(p)\n",
    "    if not models:\n",
    "        print(f\"Keine V*-Modelle in {ckpt_dir} gefunden.\")\n",
    "        sys.exit(1)\n",
    "    # Nach V-Nummer sortieren\n",
    "    models.sort(key=lambda p: int(p.stem.split('_')[0][1:]))  # V{n}_...\n",
    "    print(f\"Waehle Modell in {ckpt_dir.name}:\")\n",
    "    for i, p in enumerate(models, 1):\n",
    "        print(f\"  [{i}] {p.name}\")\n",
    "    while True:\n",
    "        s = input(\"Nummer: \").strip()\n",
    "        if s.isdigit() and 1 <= int(s) <= len(models):\n",
    "            return models[int(s)-1]\n",
    "\n",
    "# ====== Test-Dataset bauen (identisch zur Trainings-Pipeline) ======\n",
    "def build_test_dataset(use_vst: bool, size=5, group_len=41, dtype=np.float32, batch_size=4):\n",
    "    results, _ = prepare_in_memory_5to5(\n",
    "        data_dir=Path.home() / \"data\" / \"original_data\",\n",
    "        size=size,\n",
    "        group_len=group_len,\n",
    "        use_vst=use_vst,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    X_test, Y_test = results[\"test\"]\n",
    "\n",
    "    AUTO = tf.data.AUTOTUNE\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "    ds = ds.batch(batch_size).prefetch(AUTO)\n",
    "    return ds, X_test.shape[1:]\n",
    "\n",
    "# ====== Predictions einsammeln ======\n",
    "def collect_preds_and_targets(model, dataset, max_batches=None):\n",
    "    y_true, y_pred = [], []\n",
    "    for b, (xb, yb) in enumerate(dataset):\n",
    "        yhat = model.predict(xb, verbose=0)\n",
    "        y_true.append(yb.numpy())\n",
    "        y_pred.append(yhat)\n",
    "        if max_batches and (b + 1) >= max_batches:\n",
    "            break\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# ====== Hauptlogik ======\n",
    "def main():\n",
    "    ckpt_dir = pick_checkpoint_dir()\n",
    "    model_path = pick_version(ckpt_dir)\n",
    "\n",
    "    # Heuristik: wenn Ordnername \"anscombe\" enthaelt -> use_vst=True\n",
    "    use_vst = (\"anscombe\" in ckpt_dir.name.lower())\n",
    "\n",
    "    print(f\"\\n>> Lade Modell: {model_path}\")\n",
    "    # Robust laden: erklaere evtl. Custom-Objekte (egal was beim Training genutzt wurde)\n",
    "    def dummy(*args, **kwargs):  # falls nicht gebraucht\n",
    "        return None\n",
    "    custom_objects = {\n",
    "        \"combined_loss\": globals().get(\"combined_loss\", dummy),\n",
    "        \"ms_ssim_loss\": globals().get(\"ms_ssim_loss\", dummy),\n",
    "        \"ms_ssim_metric\": globals().get(\"ms_ssim_metric\", dummy),\n",
    "        \"psnr_metric\": globals().get(\"psnr_metric\", dummy),\n",
    "        \"psnr_orig_metric\": globals().get(\"psnr_orig_metric\", dummy),\n",
    "        \"ms_ssim_orig_metric\": globals().get(\"ms_ssim_orig_metric\", dummy),\n",
    "    }\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "    print(\">> Baue Test-Datasetâ€¦ (use_vst =\", use_vst, \")\")\n",
    "    test_ds, input_shape = build_test_dataset(\n",
    "        use_vst=use_vst, size=5, group_len=41, dtype=np.float32, batch_size=4\n",
    "    )\n",
    "\n",
    "    # Vorhersagen\n",
    "    Y_true, Y_pred = collect_preds_and_targets(model, test_ds, max_batches=None)\n",
    "\n",
    "    # Basis-Fehler (im Trainingsraum)\n",
    "    yt = Y_true.ravel()\n",
    "    yp = Y_pred.ravel()\n",
    "    mse  = float(np.mean((yt - yp) ** 2))\n",
    "    mae  = float(np.mean(np.abs(yt - yp)))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2   = float(r2_score(yt, yp))\n",
    "\n",
    "    # PSNR / SSIM: je nach use_vst im Originalraum\n",
    "    if use_vst:\n",
    "        Y_true_o = inv_anscombe_tf(tf.convert_to_tensor(Y_true))\n",
    "        Y_pred_o = inv_anscombe_tf(tf.convert_to_tensor(Y_pred))\n",
    "        # clamp in [0,1] fuer diese Metriken, wie in deinen Trainingsmetriken\n",
    "        Y_true_o = tf.clip_by_value(Y_true_o, 0.0, 1.0)\n",
    "        Y_pred_o = tf.clip_by_value(Y_pred_o, 0.0, 1.0)\n",
    "\n",
    "        psnr = float(tf.image.psnr(Y_true_o, Y_pred_o, max_val=1.0).numpy().mean())\n",
    "\n",
    "        Y_true_2d = Y_true_o[:, Y_true_o.shape[1] // 2, :, :, :]\n",
    "        Y_pred_2d = Y_pred_o[:, Y_pred_o.shape[1] // 2, :, :, :]\n",
    "        ssim = float(tf.image.ssim(Y_true_2d, Y_pred_2d, max_val=1.0).numpy().mean())\n",
    "    else:\n",
    "        psnr = float(tf.image.psnr(Y_true, Y_pred, max_val=1.0).numpy().mean())\n",
    "        Y_true_2d = Y_true[:, Y_true.shape[1] // 2, :, :, :]\n",
    "        Y_pred_2d = Y_pred[:, Y_pred.shape[1] // 2, :, :, :]\n",
    "        ssim = float(tf.image.ssim(Y_true_2d, Y_pred_2d, max_val=1.0).numpy().mean())\n",
    "\n",
    "    print(\"\\n=== Evaluation auf Test Set ===\")\n",
    "    print(f\"Modell       : {model_path.name}\")\n",
    "    print(f\"INPUT_SHAPE  : {input_shape}\")\n",
    "    print(f\"use_vst      : {use_vst}\")\n",
    "    print(f\"MSE          : {mse:.6f}\")\n",
    "    print(f\"MAE          : {mae:.6f}\")\n",
    "    print(f\"RMSE         : {rmse:.6f}\")\n",
    "    print(f\"R2           : {r2:.6f}\")\n",
    "    print(f\"PSNR         : {psnr:.2f} dB\")\n",
    "    print(f\"SSIM (mid-Z) : {ssim:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
