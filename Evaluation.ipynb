{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7da771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e9897b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected INPUT_SHAPE: (5, 192, 240, 1)\n"
     ]
    }
   ],
   "source": [
    "# ======== Load Test Data ========\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\", \"data_3D_U-net\")\n",
    "\n",
    "# Shape check from one file\n",
    "probe = np.load(os.path.join(DATA_DIR, \"X_test.npy\"), mmap_mode=\"r\")\n",
    "INPUT_SHAPE = tuple(probe.shape[1:])  # (D,H,W,C)\n",
    "print(\"Detected INPUT_SHAPE:\", INPUT_SHAPE)\n",
    "del probe\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def load_split(split):\n",
    "    x = np.load(os.path.join(DATA_DIR, f\"X_{split}.npy\"), mmap_mode=\"r\")\n",
    "    y = np.load(os.path.join(DATA_DIR, f\"Y_{split}.npy\"), mmap_mode=\"r\")\n",
    "    if x.shape[1:] != INPUT_SHAPE or y.shape[1:] != INPUT_SHAPE:\n",
    "        raise RuntimeError(f\"{split} shape mismatch: {x.shape[1:]} vs {INPUT_SHAPE}\")\n",
    "    return x, y\n",
    "\n",
    "X_test, Y_test = load_split(\"test\")\n",
    "\n",
    "def make_ds(X_mm, Y_mm, shuffle=False):\n",
    "    n = X_mm.shape[0]\n",
    "    idx = np.arange(n, dtype=np.int64)\n",
    "\n",
    "    def _fetch(i):\n",
    "        i = int(i)\n",
    "        return X_mm[i], Y_mm[i]\n",
    "\n",
    "    def tf_fetch(i):\n",
    "        x, y = tf.numpy_function(_fetch, [i], [tf.float32, tf.float32])\n",
    "        x.set_shape(INPUT_SHAPE)\n",
    "        y.set_shape(INPUT_SHAPE)\n",
    "        return x, y\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(idx)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(8000, n), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(tf_fetch, num_parallel_calls=AUTO)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "# Build dataset for evaluation\n",
    "test_ds = make_ds(X_test, Y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b3391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model: c:\\Users\\sandr\\VS_Master_Thesis\\checkpoints_3d_unet\\best.keras\n",
      "=== Evaluation on Test Set (loaded best checkpoint) ===\n",
      "MSE   : 0.000377\n",
      "MAE   : 0.011563\n",
      "RMSE  : 0.019413\n",
      "R2    : 0.716394\n",
      "PSNR  : 38.20 dB\n",
      "SSIM  : 0.9145\n"
     ]
    }
   ],
   "source": [
    "# ======== Load best saved model & evaluate on test ========\n",
    "\n",
    "# Open Model that should be evaluated\n",
    "ckpt_dir = os.path.join(os.getcwd(), \"checkpoints_3d_unet\")\n",
    "best_path = os.path.join(ckpt_dir, \"best.keras\") # Pick model to evaluate\n",
    "\n",
    "print(f\"Load Model: {best_path}\")\n",
    "best_model = tf.keras.models.load_model(best_path, custom_objects={\n",
    "        \"combined_loss\": globals().get(\"combined_loss\"),\n",
    "        \"ms_ssim_loss\": globals().get(\"ms_ssim_loss\"),\n",
    "        \"ms_ssim_metric\": globals().get(\"ms_ssim_metric\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Collect all predictions and targets from the test dataset\n",
    "def collect_preds_and_targets(model, dataset, max_batches=None):\n",
    "    y_true, y_pred = [], []\n",
    "    for b, (xb, yb) in enumerate(dataset):\n",
    "        yhat = model.predict(xb, verbose=0)\n",
    "        y_true.append(yb.numpy())\n",
    "        y_pred.append(yhat)\n",
    "        if max_batches and (b + 1) >= max_batches:\n",
    "            break\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Optional: testing less batches, e.g.. MAX_BATCHES = 50\n",
    "MAX_BATCHES = None\n",
    "\n",
    "# Evaluation\n",
    "Y_true, Y_pred = collect_preds_and_targets(best_model, test_ds, max_batches=MAX_BATCHES)\n",
    "\n",
    "yt = Y_true.ravel()\n",
    "yp = Y_pred.ravel()\n",
    "\n",
    "mse  = np.mean((yt - yp) ** 2)\n",
    "mae  = np.mean(np.abs(yt - yp))\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(yt, yp)\n",
    "\n",
    "# PSNR over 3D-Volume (Data in [0,1])\n",
    "psnr = tf.image.psnr(Y_true, Y_pred, max_val=1.0).numpy().mean()\n",
    "\n",
    "# SSIM slice wise (mittlerer Slice entlang D)\n",
    "Y_true_2d = Y_true[:, Y_true.shape[1] // 2, :, :, :]\n",
    "Y_pred_2d = Y_pred[:, Y_pred.shape[1] // 2, :, :, :]\n",
    "ssim = tf.image.ssim(Y_true_2d, Y_pred_2d, max_val=1.0).numpy().mean()\n",
    "\n",
    "print(\"=== Evaluation on Test Set (loaded best checkpoint) ===\")\n",
    "print(f\"MSE   : {mse:.6f}\")\n",
    "print(f\"MAE   : {mae:.6f}\")\n",
    "print(f\"RMSE  : {rmse:.6f}\")\n",
    "print(f\"R2    : {r2:.6f}\")\n",
    "print(f\"PSNR  : {psnr:.2f} dB\")\n",
    "print(f\"SSIM  : {ssim:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
