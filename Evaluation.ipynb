{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc70fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from unet_3d_data import prepare_in_memory_5to5\n",
    "\n",
    "DATA_ROOT = Path.home() / \"data\"\n",
    "EVAL_ROOT = DATA_ROOT  # Output directory\n",
    "\n",
    "# ===== Anscombe utils (for original-domain metrics) =====\n",
    "def _sanitize_name(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in (s or \"\").strip()) or \"EVAL\"\n",
    "\n",
    "def _auto_script_name() -> str:\n",
    "    # Name dieses Eval-Skripts ohne .py\n",
    "    try:\n",
    "        path = sys.modules.get(\"__main__\").__file__\n",
    "    except Exception:\n",
    "        path = None\n",
    "    if not path:\n",
    "        path = sys.argv[0] if sys.argv else \"eval\"\n",
    "    return _sanitize_name(os.path.splitext(os.path.basename(path))[0])\n",
    "\n",
    "def inv_anscombe_tf(z, eps=1e-6):\n",
    "    \"\"\"Approximate unbiased inverse of Anscombe; clamp small z for stability.\"\"\"\n",
    "    z = tf.maximum(z, eps)\n",
    "    z2 = z / 2.0\n",
    "    y = z2**2 - 1.0/8.0 + 1.0/(4.0*z**2) - 11.0/(8.0*z**4)\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "def stable_psnr(y_true, y_pred, eps=1e-12):\n",
    "    \"\"\"\n",
    "    PSNR computed from per-sample MSE with epsilon for stability.\n",
    "    y_*: Tensor [B, D, H, W, C] (or similar), values expected in [0,1].\n",
    "    \"\"\"\n",
    "    err2 = tf.square(y_true - y_pred)\n",
    "    axes = tf.range(1, tf.rank(err2))                     # mean over all but batch\n",
    "    mse = tf.reduce_mean(err2, axis=axes)\n",
    "    psnr = -10.0 * tf.math.log(mse + eps) / tf.math.log(tf.constant(10.0, dtype=mse.dtype))\n",
    "    return tf.reduce_mean(psnr)\n",
    "\n",
    "def compute_ms_ssim(Y_true, Y_pred):\n",
    "    # (B, D, H, W, C) -> zu (B*D, H, W, C)\n",
    "    yt2 = tf.reshape(Y_true, (-1, tf.shape(Y_true)[2], tf.shape(Y_true)[3], tf.shape(Y_true)[4]))\n",
    "    yp2 = tf.reshape(Y_pred, (-1, tf.shape(Y_pred)[2], tf.shape(Y_pred)[3], tf.shape(Y_pred)[4]))\n",
    "    return float(tf.image.ssim_multiscale(yt2, yp2, max_val=1.0).numpy().mean())\n",
    "\n",
    "# ===== Selection helpers =====\n",
    "def pick_checkpoint_dir():\n",
    "    # sucht unter ~/data nach Verzeichnissen \"checkpoints_*\"\n",
    "    if not DATA_ROOT.exists():\n",
    "        print(f\"{DATA_ROOT} existiert nicht.\")\n",
    "        sys.exit(1)\n",
    "    cand = sorted([p for p in DATA_ROOT.iterdir()\n",
    "                   if p.is_dir() and p.name.startswith(\"checkpoints_\")])\n",
    "    if not cand:\n",
    "        print(\"Keine Checkpoint-Ordner gefunden unter ~/data (checkpoints_*)\")\n",
    "        sys.exit(1)\n",
    "    print(\"Waehle Checkpoint-Ordner:\")\n",
    "    for i, p in enumerate(cand, 1):\n",
    "        print(f\"  [{i}] {p.name}\")\n",
    "    while True:\n",
    "        s = input(\"Nummer: \").strip()\n",
    "        if s.isdigit():\n",
    "            idx = int(s)\n",
    "            if 1 <= idx <= len(cand):\n",
    "                return cand[idx - 1]\n",
    "\n",
    "def pick_version(ckpt_dir: Path):\n",
    "    models = []\n",
    "    pat = re.compile(r\"^V(\\d+)_.*\\.keras$\")\n",
    "    for p in ckpt_dir.iterdir():\n",
    "        if p.is_file() and p.suffix == \".keras\" and pat.match(p.name):\n",
    "            models.append(p)\n",
    "    if not models:\n",
    "        print(f\"Keine V*-Modelle in {ckpt_dir} gefunden.\")\n",
    "        sys.exit(1)\n",
    "    # sort by V number\n",
    "    models.sort(key=lambda p: int(p.stem.split('_')[0][1:]))\n",
    "    print(f\"Waehle Modell in {ckpt_dir.name}:\")\n",
    "    for i, p in enumerate(models, 1):\n",
    "        print(f\"  [{i}] {p.name}\")\n",
    "    while True:\n",
    "        s = input(\"Nummer: \").strip()\n",
    "        if s.isdigit():\n",
    "            idx = int(s)\n",
    "            if 1 <= idx <= len(models):\n",
    "                return models[idx - 1]\n",
    "\n",
    "# ===== Determine use_vst (from JSON; fallback: folder name) =====\n",
    "def detect_use_vst(model_path: Path) -> bool:\n",
    "    meta_path = model_path.with_suffix(\".json\")\n",
    "    if meta_path.exists():\n",
    "        try:\n",
    "            with open(meta_path, \"r\") as f:\n",
    "                meta = json.load(f)\n",
    "            return bool(meta.get(\"data_prep\", {}).get(\"use_vst\", False))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return (\"anscombe\" in model_path.parent.name.lower())\n",
    "\n",
    "\n",
    "# ===== Test dataset =====\n",
    "# ÄNDERUNG: Funktion so anpassen, dass sie das Metadaten-Dict zurückgibt\n",
    "def build_test_dataset(use_vst: bool, size=5, group_len=41, dtype=np.float32, batch_size=4):\n",
    "    results, meta = prepare_in_memory_5to5( # <-- meta empfangen\n",
    "        data_dir=Path.home() / \"data\" / \"original_data\",\n",
    "        size=size,\n",
    "        group_len=group_len,\n",
    "        use_vst=use_vst,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    X_test, Y_test = results[\"test\"]\n",
    "    AUTO = tf.data.AUTOTUNE\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "    ds = ds.batch(batch_size).prefetch(AUTO)\n",
    "    # ÄNDERUNG: meta zurückgeben\n",
    "    return ds, X_test.shape[1:], meta\n",
    "\n",
    "# ===== Collect predictions =====\n",
    "def collect_preds_and_targets(model, dataset, max_batches=None):\n",
    "    y_true, y_pred = [], []\n",
    "    for b, (xb, yb) in enumerate(dataset):\n",
    "        yhat = model.predict(xb, verbose=0)\n",
    "        y_true.append(yb.numpy())\n",
    "        y_pred.append(yhat)\n",
    "        if max_batches and (b + 1) >= max_batches:\n",
    "            break\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# ===== val_loss helpers fuer Dateinamen =====\n",
    "def _read_val_loss_from_meta(model_path: Path) -> Optional[float]:\n",
    "    \"\"\"Versucht val_loss aus einer nebenliegenden JSON-Meta zu lesen.\"\"\"\n",
    "    meta_path = model_path.with_suffix(\".json\")\n",
    "    if not meta_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        if isinstance(meta, dict):\n",
    "            if \"val_loss\" in meta:\n",
    "                return float(meta[\"val_loss\"])\n",
    "            if \"best_val_loss\" in meta:\n",
    "                return float(meta[\"best_val_loss\"])\n",
    "            hist = meta.get(\"history\", {})\n",
    "            if isinstance(hist, dict) and \"val_loss\" in hist and hist[\"val_loss\"]:\n",
    "                try:\n",
    "                    return float(np.min(hist[\"val_loss\"]))\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        return float(hist[\"val_loss\"][-1])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _read_val_loss_from_name(model_path: Path) -> Optional[float]:\n",
    "    \"\"\"Extrahiert val_loss aus dem Dateinamen (unterstuetzt 'val_loss=' oder 'valloss=').\"\"\"\n",
    "    stem = model_path.stem.lower()\n",
    "    m = re.search(r\"(?:val[_-]?loss|valloss)\\s*=\\s*([0-9]*\\.?[0-9]+)\", stem)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _build_eval_filename(model_path: Path, psnr_value: float, val_loss_value: Optional[float], prefix: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Baut: <prefix>_<modellstem>_val<loss>_psnr<db>.json\n",
    "    prefix: Standard = Name des Eval-Skripts.\n",
    "    \"\"\"\n",
    "    stem = model_path.stem\n",
    "    pref = _sanitize_name(prefix) if prefix else _auto_script_name()\n",
    "    if val_loss_value is not None:\n",
    "        return f\"{pref}_{stem}_val{val_loss_value:.6f}_psnr{psnr_value:.2f}.json\"\n",
    "    else:\n",
    "        return f\"{pref}_{stem}_psnr{psnr_value:.2f}.json\"\n",
    "\n",
    "# ====== Ergebnisse speichern ======\n",
    "def save_results(model_path, results: dict):\n",
    "    # Zielordner: ~/data/model_evaluations\n",
    "    out_dir = EVAL_ROOT / \"model_evaluations\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # val_loss beschaffen (Meta -> Name -> None)\n",
    "    val_loss = _read_val_loss_from_meta(model_path)\n",
    "    if val_loss is None:\n",
    "        val_loss = _read_val_loss_from_name(model_path)\n",
    "\n",
    "    # PSNR fuer den Dateinamen\n",
    "    psnr_value = float(results.get(\"psnr\", 0.0))\n",
    "\n",
    "    # Dateiname bauen\n",
    "    out_name = _build_eval_filename(model_path, psnr_value, val_loss)  # prefix auto aus Skriptname\n",
    "    out_path = out_dir / out_name\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n>> Ergebnisse gespeichert unter: {out_path}\")\n",
    "\n",
    "# ===== Main =====\n",
    "def main():\n",
    "    ckpt_dir = pick_checkpoint_dir()\n",
    "    model_path = pick_version(ckpt_dir)\n",
    "\n",
    "    use_vst = detect_use_vst(model_path)\n",
    "    print(f\"\\n>> Lade Modell: {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "    print(\">> Baue Test-Dataset… (use_vst =\", use_vst, \")\")\n",
    "    # ÄNDERUNG: meta hier empfangen\n",
    "    test_ds, input_shape, data_meta = build_test_dataset(\n",
    "        use_vst=use_vst, size=5, group_len=41, dtype=np.float32, batch_size=4\n",
    "    )\n",
    "\n",
    "    Y_true, Y_pred = collect_preds_and_targets(model, test_ds, max_batches=None)\n",
    "    \n",
    "    if use_vst:\n",
    "        clip_val = data_meta.get(\"clip_val\")\n",
    "        if clip_val is None:\n",
    "            print(\"[FEHLER] clip_val nicht in data_meta gefunden. Abbruch.\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        print(f\">> Wende inverse Transformation an mit clip_val = {clip_val:.4f}\")\n",
    "        \n",
    "        # 1. Skalierung rückgängig machen, um rohe VST-Werte zu erhalten\n",
    "        Y_true_unscaled = tf.convert_to_tensor(Y_true) * clip_val\n",
    "        Y_pred_unscaled = tf.convert_to_tensor(Y_pred) * clip_val\n",
    "        \n",
    "        # 2. Inverse Transformation auf unskalierten Werten anwenden\n",
    "        # HINWEIS: Die Normalisierung des Originalraums (z.B. durch einen anderen clip_val)\n",
    "        # ist hier implizit, da die inv_anscombe-Funktion Werte im \"Zähl\"-Raum erzeugt,\n",
    "        # die aber durch die ursprüngliche Normalisierung in `unet_3d_data` bereits\n",
    "        # so skaliert sind, dass sie nach der Rücktransformation wieder grob im Bereich [0,1] liegen sollten.\n",
    "        # Für eine 100% saubere Trennung müsste man den clip_val für den VST-Raum und\n",
    "        # den clip_val für den Original-Raum getrennt berechnen und übergeben.\n",
    "        # Für den Moment ist diese Vereinfachung aber der richtige Weg.\n",
    "        \n",
    "        Y_true_o = inv_anscombe_tf(Y_true_unscaled)\n",
    "        Y_pred_o = inv_anscombe_tf(Y_pred_unscaled)\n",
    "        \n",
    "        # 3. Clipping zur Stabilisierung\n",
    "        Y_true_o = tf.clip_by_value(Y_true_o, 0.0, 1.0)\n",
    "        Y_pred_o = tf.clip_by_value(Y_pred_o, 0.0, 1.0)\n",
    "        \n",
    "        Y_true_m = Y_true_o.numpy()\n",
    "        Y_pred_m = Y_pred_o.numpy()\n",
    "    else:\n",
    "        Y_true_m = Y_true\n",
    "        Y_pred_m = Y_pred\n",
    "\n",
    "    yt = Y_true_m.ravel()\n",
    "    yp = Y_pred_m.ravel()\n",
    "    mse  = float(np.mean((yt - yp) ** 2))\n",
    "    mae  = float(np.mean(np.abs(yt - yp)))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "\n",
    "    psnr = float(tf.image.psnr(Y_true_m, Y_pred_m, max_val=1.0).numpy().mean())\n",
    "    # Mittel-Slice vorbereitet (optional nutzbar)\n",
    "    Y_true_2d = Y_true_m[:, Y_true_m.shape[1] // 2, :, :, :]\n",
    "    Y_pred_2d = Y_pred_m[:, Y_pred_m.shape[1] // 2, :, :, :]\n",
    "    ms_ssim = compute_ms_ssim(Y_true_m, Y_pred_m)\n",
    "\n",
    "    print(\"\\n=== Evaluation auf Test Set (Originalraum) ===\")\n",
    "    print(f\"Modell       : {model_path.name}\")\n",
    "    print(f\"INPUT_SHAPE  : {input_shape}\")\n",
    "    print(f\"use_vst      : {use_vst}\")\n",
    "    print(f\"MSE          : {mse:.6f}\")\n",
    "    print(f\"MAE          : {mae:.6f}\")\n",
    "    print(f\"RMSE         : {rmse:.6f}\")\n",
    "    print(f\"PSNR         : {psnr:.2f} dB\")\n",
    "    print(f\"MS-SSIM      : {ms_ssim:.4f}\")\n",
    "\n",
    "    # ---- Ergebnisse abspeichern ----\n",
    "    results = {\n",
    "        \"model\": model_path.name,\n",
    "        \"input_shape\": tuple(int(x) for x in input_shape),\n",
    "        \"use_vst\": use_vst,\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"psnr\": psnr,\n",
    "        \"ms_ssim\": ms_ssim,\n",
    "    }\n",
    "    save_results(model_path, results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nAbgebrochen.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
