{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Imports =======\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\") # Increases performance without loss of quality (calculations still done with float_32 precision)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from unet_3d_data import prepare_in_memory_5to5\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Allocate GPU memory dynamically as needed =======\n",
    "for g in tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE # Chooses optimal number of threads automatically depending on hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Loading Data in RAM =====\n",
    "\n",
    "print(\">>> Phase 1: Starting data prep on CPU...\")\n",
    "(results, size) = prepare_in_memory_5to5(\n",
    "    data_dir=Path.home() / \"data\" / \"original_data\",\n",
    "    use_vst=True, # With anscombe transform\n",
    "    size=5,\n",
    "    group_len=41,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "print(\">>> Data preperation finished, all data in RAM\")\n",
    "\n",
    "X_train, Y_train = results[\"train\"]\n",
    "X_val,   Y_val   = results[\"val\"]\n",
    "X_test,  Y_test  = results[\"test\"]\n",
    "\n",
    "\n",
    "INPUT_SHAPE = X_train.shape[1:]  # (5, H, W, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42733def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Making Tensorflow dataset =======\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 400\n",
    "\n",
    "# Sanity check for INPUT_SHAPE\n",
    "D,H,W,C = INPUT_SHAPE\n",
    "if (H % 8) or (W % 8):\n",
    "    print(f\"[WARN] H={H} oder W={W} nicht durch 8 teilbar (3x (1,2,2)-Pooling)\")\n",
    "\n",
    "def make_ds(X, Y, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a tensorflow dataset\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=X.shape[0])\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "print(\">>> Phase 2: Create Tensorflow Datasets...\")\n",
    "train_ds = make_ds(X_train, Y_train, True)\n",
    "val_ds   = make_ds(X_val,   Y_val,   False)\n",
    "test_ds  = make_ds(X_test,  Y_test,  False)\n",
    "print(\">>> Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Defining 3D-U-Net Architecture ========\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3,3,3), padding=\"same\", activation=\"relu\"):\n",
    "    x = layers.Conv3D(filters, kernel_size, padding=padding)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv3D(filters, kernel_size, padding=padding)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def unet3d(input_shape=(5, 192, 240, 1), base_filters=32):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder (pool only over H,W)\n",
    "    c1 = conv_block(inputs, base_filters)\n",
    "    p1 = layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2))(c1)\n",
    "\n",
    "    c2 = conv_block(p1, base_filters*2)\n",
    "    p2 = layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2))(c2)\n",
    "\n",
    "    c3 = conv_block(p2, base_filters*4)\n",
    "    p3 = layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = conv_block(p3, base_filters*8)\n",
    "\n",
    "    # Decoder (upsample only over H,W)\n",
    "    u3 = layers.Conv3DTranspose(base_filters*4, kernel_size=(1,2,2), strides=(1,2,2), padding=\"same\")(bn) # bottleneck\n",
    "    u3 = layers.concatenate([u3, c3])\n",
    "    c4 = conv_block(u3, base_filters*4)\n",
    "\n",
    "    u2 = layers.Conv3DTranspose(base_filters*2, kernel_size=(1,2,2), strides=(1,2,2), padding=\"same\")(c4)\n",
    "    u2 = layers.concatenate([u2, c2])\n",
    "    c5 = conv_block(u2, base_filters*2)\n",
    "\n",
    "    u1 = layers.Conv3DTranspose(base_filters, kernel_size=(1,2,2), strides=(1,2,2), padding=\"same\")(c5)\n",
    "    u1 = layers.concatenate([u1, c1])\n",
    "    c6 = conv_block(u1, base_filters)\n",
    "\n",
    "    outputs = layers.Conv3D(1, (1,1,1), dtype=\"float32\", activation=None)(c6)  # linear activation\n",
    "    return models.Model(inputs, outputs, name=\"3D_U-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Defining Metrics for training ========\n",
    "\n",
    "def inv_anscombe(z):\n",
    "    z = tf.maximum(z, 1e-3)\n",
    "    z2 = z / 2.0\n",
    "    # Unbiased approx. (gekÃ¼rzt) fuer Poisson\n",
    "    y = z2**2 - 1.0/8.0 + 1.0/(4.0*z**2) - 11.0/(8.0*z**4)\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "def psnr_orig_metric(y_true_vst, y_pred_vst):\n",
    "    y_true = tf.clip_by_value(inv_anscombe(y_true_vst), 0.0, 1.0)\n",
    "    y_pred = tf.clip_by_value(inv_anscombe(y_pred_vst), 0.0, 1.0)\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "def ms_ssim_orig_metric(y_true_vst, y_pred_vst):\n",
    "    yt = inv_anscombe(y_true_vst)\n",
    "    yp = inv_anscombe(y_pred_vst)\n",
    "    yt2 = tf.reshape(yt, (-1, tf.shape(yt)[2], tf.shape(yt)[3], tf.shape(yt)[4]))\n",
    "    yp2 = tf.reshape(yp, (-1, tf.shape(yp)[2], tf.shape(yp)[3], tf.shape(yp)[4]))\n",
    "    return tf.reduce_mean(tf.image.ssim_multiscale(yt2, yp2, max_val=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Callbacks =======\n",
    "\n",
    "ckpt_dir = os.path.expanduser(\"~/data/checkpoints_3d_unet\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(os.path.join(ckpt_dir, \"best_V2.keras\"), monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=0),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Train =======\n",
    "\n",
    "print(\">>> Phase 3: GPU training starts now!\")\n",
    "\n",
    "model = unet3d(input_shape=INPUT_SHAPE, base_filters=16)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\", \"mse\", psnr_orig_metric, ms_ssim_orig_metric],\n",
    "    jit_compile=False # Would be false per default, but just to be sure\n",
    ")\n",
    "# model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cbs,\n",
    "    verbose=2\n",
    ")\n",
    "print(\">>> Training complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
